from __future__ import annotations

from typing import Dict, Any

import torch
from torch import Tensor

# 只是为了满足框架结构，我们仍然 import 这些
from isaacgym import gymapi
from .base.vec_task import VecTask


class DrivingTrajectoryTask(VecTask):
    """
    A vectorized trajectory-planning task that does NOT rely on Isaac Gym
    physics simulation.

    - Observation: arbitrary features for each scenario (current ego state, map, etc.)
    - Action: a full future trajectory generated by the policy / model
    - Reward: computed purely from the generated trajectories

    You only need to customize:
        * _compute_observations()
        * _compute_reward()
        * reset_idx()
    """

    def __init__(self, cfg: Dict[str, Any], sim_device: str, headless: bool):
        # ------------------------------------------------------------------ #
        # 1. 解析配置
        # ------------------------------------------------------------------ #
        self.cfg = cfg
        self.device = torch.device(sim_device)

        env_cfg = cfg["env"]
        # 并行环境个数
        self.num_envs: int = env_cfg.get("numEnvs", 1024)

        # 规划步长（未来多少步）
        self.horizon: int = env_cfg.get("planning_horizon", 16)

        # 每个时间步的轨迹维度：例如 (x, y) → 2；如果你有 (x,y,yaw,v) 就改成 4
        self.per_step_action_dim: int = env_cfg.get("per_step_action_dim", 2)

        # 整个 action 向量维度：一次输出完整轨迹
        self.num_actions: int = self.horizon * self.per_step_action_dim

        # 观测维度，可以根据需要在 _compute_observations 中真正决定
        self.num_obs: int = env_cfg.get("numObservations", 32)

        # episode 长度（多少次 step 后强制 reset）
        self.max_episode_length: int = env_cfg.get("episodeLength", 1)

        # ------------------------------------------------------------------ #
        # 2. 调用 VecTask 的构造函数（注意 rl_device 和 sim_device 传一样）
        # ------------------------------------------------------------------ #
        super().__init__(
            cfg=cfg,
            rl_device=sim_device,
            sim_device=sim_device,
            headless=headless,
        )

        # ------------------------------------------------------------------ #
        # 3. 分配需要的 buffer
        # ------------------------------------------------------------------ #
        self.obs_buf = torch.zeros(
            self.num_envs, self.num_obs, device=self.device, dtype=torch.float32
        )
        self.rew_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.float32)
        self.reset_buf = torch.ones(self.num_envs, device=self.device, dtype=torch.long)
        self.progress_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)

        # 存放当前场景的信息（例如起点、终点等），这里用占位符
        self.current_start = torch.zeros(self.num_envs, 2, device=self.device)
        self.current_goal = torch.zeros(self.num_envs, 2, device=self.device)

    # ---------------------------------------------------------------------- #
    # 必须实现的 VecTask 接口
    # ---------------------------------------------------------------------- #

    def create_sim(self):
        """
        Dummy simulator creation.

        We still create an empty Isaac Gym simulation to keep VecTask happy,
        but we never actually step the physics engine.
        """
        # minimal dummy sim
        self.gym = gymapi.acquire_gym()
        sim_params = gymapi.SimParams()
        sim_params.up_axis = gymapi.UP_AXIS_Z
        sim_params.use_gpu_pipeline = False

        # create a trivial sim with no assets
        self.sim = self.gym.create_sim(0, 0, gymapi.SIM_PHYSX, sim_params)

        # we don't create any envs / actors because we don't use the physics

    def pre_physics_step(self, actions: Tensor):
        """
        In this task, 'actions' are complete future trajectories.

        actions: [num_envs, num_actions]
            → reshape to [num_envs, horizon, per_step_action_dim]
        """
        # 保存当前动作用于计算 reward
        self.actions = actions.view(
            self.num_envs, self.horizon, self.per_step_action_dim
        ).to(self.device)

    def post_physics_step(self):
        """
        We do NOT run any physics. We only:
          1) compute rewards based on trajectories
          2) update observations
          3) decide which envs to reset
        """
        # 1) 计算奖励 & 是否结束
        self._compute_reward()

        # 2) 更新 progress
        self.progress_buf += 1
        # episode 长度一到就强制 reset
        done_due_to_len = self.progress_buf >= self.max_episode_length
        self.reset_buf |= done_due_to_len

        # 3) 重置需要重置的环境
        env_ids = torch.nonzero(self.reset_buf, as_tuple=False).squeeze(-1)
        if env_ids.numel() > 0:
            self.reset_idx(env_ids)

        # 4) 更新观测
        self._compute_observations()

    def reset_idx(self, env_ids: Tensor):
        """
        Reset a subset of environments.
        Here you can:
          * sample a new scenario from dataset
          * set new start / goal
          * reset progress / buffers
        """
        if env_ids.numel() == 0:
            return

        # 示例：随机采样新的起点和终点（你可以改成从数据集中读）
        self.current_start[env_ids] = torch.rand(
            len(env_ids), 2, device=self.device
        ) * 10.0  # [0, 10) 范围
        self.current_goal[env_ids] = torch.rand(
            len(env_ids), 2, device=self.device
        ) * 10.0

        self.progress_buf[env_ids] = 0
        self.reset_buf[env_ids] = 0
        self.rew_buf[env_ids] = 0.0

        # 初始化观测
        self._compute_observations(env_ids=env_ids)

    # ---------------------------------------------------------------------- #
    # 额外：我们自己实现一个 step，完全不调用物理仿真
    # （如果你想沿用 VecTask 默认的 step，可以删掉这个函数）
    # ---------------------------------------------------------------------- #
    def step(self, actions: Tensor):
        """
        Overwrite VecTask.step so that we don't call self.gym.simulate().

        RL-Games will call this method.
        """
        self.pre_physics_step(actions)
        self.post_physics_step()
        return self.obs_buf, self.rew_buf, self.reset_buf, {}

    # ---------------------------------------------------------------------- #
    # 辅助函数：计算观测 & reward（需要你自己改成真正的任务）
    # ---------------------------------------------------------------------- #

    def _compute_observations(self, env_ids: Tensor | None = None):
        """
        根据当前场景信息构造观测。

        在实际项目中，你可以把:
          * 当前 ego pose
          * 邻车信息
          * 局部地图编码
        都拼到 obs_buf 里。
        """
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=self.device)

        # 这里给出一个最简单的占位示例：
        # obs = [start_x, start_y, goal_x, goal_y, ...zeros...]
        obs = torch.zeros(len(env_ids), self.num_obs, device=self.device)
        obs[:, 0:2] = self.current_start[env_ids]
        obs[:, 2:4] = self.current_goal[env_ids]

        self.obs_buf[env_ids] = obs

    def _compute_reward(self):
        """
        根据生成的轨迹 self.actions 计算 reward。

        你可以把 Waymo / nuPlan / NavSim 的各种指标封装到这里：
          * 到终点的距离
          * 与碰撞区域的最小距离
          * 速度平滑性
          * 车道偏离等
        """
        # self.actions: [num_envs, horizon, per_step_action_dim]
        traj_xy = self.actions[..., 0:2]  # 只取 (x, y)

        # 1) 终点距离：轨迹最后一个点离 goal 越近越好
        final_pos = traj_xy[:, -1, :]  # [N, 2]
        goal = self.current_goal
        dist_to_goal = torch.norm(final_pos - goal, dim=-1)  # [N]
        reward_goal = -dist_to_goal  # 越小越好

        # 2) 轨迹平滑性：相邻步之间差分的平方和
        diffs = traj_xy[:, 1:, :] - traj_xy[:, :-1, :]
        smooth_cost = (diffs ** 2).sum(dim=[1, 2])  # [N]
        reward_smooth = -0.1 * smooth_cost

        # 3) 示例：虚构碰撞惩罚，你可以改成真实的 collision checker
        collision_penalty = torch.zeros_like(reward_goal)

        # 总奖励
        self.rew_buf = reward_goal + reward_smooth + collision_penalty

        # 如果你希望一条轨迹就结束一个 episode，可以直接：
        # self.reset_buf[:] = 1
        # 这里我们用 max_episode_length 控制

    # ---------------------------------------------------------------------- #
    # IsaacGymEnvs 要求的接口：返回 obs/act 数量
    # ---------------------------------------------------------------------- #
    def get_obs_size(self):
        return self.num_obs

    def get_action_size(self):
        return self.num_actions

    def get_number_of_agents(self):
        # 单智能体任务
        return 1
